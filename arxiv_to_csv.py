import csv
import os
import time
from datetime import datetime
from typing import List, Dict, Optional
from arxiv_fetcher import ArxivFetcher

class ArxivToCSV:
    """arXiv è®ºæ–‡æ•°æ® CSV å­˜å‚¨å™¨"""

    def __init__(self, csv_filename: str = "arxiv_papers.csv"):
        """
        åˆå§‹åŒ– CSV å­˜å‚¨å™¨

        Args:
            csv_filename: CSV æ–‡ä»¶å
        """
        self.csv_filename = csv_filename
        self.fieldnames = [
            'arxiv_id',
            'title',
            'authors',
            'summary',
            'published_date',
            'updated_date',
            'categories',
            'pdf_url',
            'source_url',
            'created_at'
        ]
        self._init_csv()

    def _init_csv(self):
        """åˆå§‹åŒ– CSV æ–‡ä»¶ï¼Œå†™å…¥è¡¨å¤´"""
        if not os.path.exists(self.csv_filename):
            with open(self.csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)
                writer.writeheader()
            print(f"ğŸ“ åˆ›å»ºæ–°çš„ CSV æ–‡ä»¶: {self.csv_filename}")

    def _load_existing_ids(self) -> set:
        """åŠ è½½ CSV æ–‡ä»¶ä¸­å·²å­˜åœ¨çš„è®ºæ–‡ ID"""
        if not os.path.exists(self.csv_filename):
            return set()

        existing_ids = set()
        try:
            with open(self.csv_filename, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    if row.get('arxiv_id'):
                        existing_ids.add(row['arxiv_id'])
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç°æœ‰ CSV æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return set()

        return existing_ids

    def save_to_csv(self, papers: List[Dict], append_mode: bool = True):
        """
        å°†è®ºæ–‡æ•°æ®ä¿å­˜åˆ° CSV æ–‡ä»¶

        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            append_mode: æ˜¯å¦è¿½åŠ æ¨¡å¼ï¼ˆTrue=è¿½åŠ ï¼ŒFalse=è¦†ç›–ï¼‰
        """
        if not papers:
            print("âš ï¸ æ²¡æœ‰è®ºæ–‡æ•°æ®éœ€è¦ä¿å­˜")
            return

        # æ£€æŸ¥é‡å¤
        if append_mode:
            existing_ids = self._load_existing_ids()
            new_papers = []
            duplicate_count = 0

            for paper in papers:
                if paper.get('arxiv_id') not in existing_ids:
                    new_papers.append(paper)
                    existing_ids.add(paper['arxiv_id'])
                else:
                    duplicate_count += 1

            if duplicate_count > 0:
                print(f"ğŸ“Š è·³è¿‡ {duplicate_count} ç¯‡é‡å¤è®ºæ–‡")

            papers = new_papers

        if not papers:
            print("ğŸ“„ æ‰€æœ‰è®ºæ–‡éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€ä¿å­˜")
            return

        # å†™å…¥ CSV
        mode = 'a' if append_mode else 'w'
        with open(self.csv_filename, mode, newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=self.fieldnames)

            # å¦‚æœæ˜¯è¦†ç›–æ¨¡å¼æˆ–æ–°æ–‡ä»¶ï¼Œå†™å…¥è¡¨å¤´
            if mode == 'w' or csvfile.tell() == 0:
                writer.writeheader()

            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            for paper in papers:
                # æ¸…ç†å’Œæ ¼å¼åŒ–æ•°æ®
                row = {
                    'arxiv_id': paper.get('arxiv_id', ''),
                    'title': paper.get('title', '').replace('\n', ' ').strip(),
                    'authors': '; '.join(paper.get('authors', [])),
                    'summary': paper.get('summary', '').replace('\n', ' ').strip(),
                    'published_date': paper.get('published', ''),
                    'updated_date': paper.get('updated', ''),
                    'categories': '; '.join(paper.get('categories', [])),
                    'pdf_url': paper.get('pdf_url', ''),
                    'source_url': paper.get('id', ''),
                    'created_at': current_time
                }
                writer.writerow(row)

        print(f"âœ… æˆåŠŸä¿å­˜ {len(papers)} ç¯‡è®ºæ–‡åˆ° {self.csv_filename}")

    def load_from_csv(self) -> List[Dict]:
        """
        ä» CSV æ–‡ä»¶åŠ è½½è®ºæ–‡æ•°æ®

        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        if not os.path.exists(self.csv_filename):
            print(f"âš ï¸ æ–‡ä»¶ {self.csv_filename} ä¸å­˜åœ¨")
            return []

        papers = []
        try:
            with open(self.csv_filename, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    # è½¬æ¢æ•°æ®æ ¼å¼
                    paper = {
                        'arxiv_id': row.get('arxiv_id', ''),
                        'title': row.get('title', ''),
                        'authors': row.get('authors', '').split('; ') if row.get('authors') else [],
                        'summary': row.get('summary', ''),
                        'published': row.get('published_date', ''),
                        'updated': row.get('updated_date', ''),
                        'categories': row.get('categories', '').split('; ') if row.get('categories') else [],
                        'pdf_url': row.get('pdf_url', ''),
                        'id': row.get('source_url', ''),
                        'created_at': row.get('created_at', '')
                    }
                    papers.append(paper)

            print(f"ğŸ“– ä» {self.csv_filename} åŠ è½½äº† {len(papers)} ç¯‡è®ºæ–‡")

        except Exception as e:
            print(f"âŒ è¯»å– CSV æ–‡ä»¶å¤±è´¥: {e}")

        return papers

    def search_in_csv(self, keyword: str) -> List[Dict]:
        """
        åœ¨ CSV æ•°æ®ä¸­æœç´¢å…³é”®è¯

        Args:
            keyword: æœç´¢å…³é”®è¯

        Returns:
            åŒ¹é…çš„è®ºæ–‡åˆ—è¡¨
        """
        papers = self.load_from_csv()
        if not papers:
            return []

        keyword_lower = keyword.lower()
        matched_papers = []

        for paper in papers:
            # åœ¨æ ‡é¢˜ã€æ‘˜è¦ã€ä½œè€…ã€ç±»åˆ«ä¸­æœç´¢
            search_text = ' '.join([
                paper['title'],
                paper['summary'],
                ' '.join(paper['authors']),
                ' '.join(paper['categories'])
            ]).lower()

            if keyword_lower in search_text:
                matched_papers.append(paper)

        print(f"ğŸ” æœç´¢ '{keyword}' æ‰¾åˆ° {len(matched_papers)} ç¯‡ç›¸å…³è®ºæ–‡")
        return matched_papers

    def get_statistics(self) -> Dict:
        """
        è·å– CSV æ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not os.path.exists(self.csv_filename):
            return {"total_papers": 0, "message": "æ–‡ä»¶ä¸å­˜åœ¨"}

        papers = self.load_from_csv()
        if not papers:
            return {"total_papers": 0, "message": "æ–‡ä»¶ä¸ºç©º"}

        # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
        category_count = {}
        for paper in papers:
            for category in paper['categories']:
                category_count[category] = category_count.get(category, 0) + 1

        # ç»Ÿè®¡æ—¶é—´åˆ†å¸ƒï¼ˆæŒ‰å¹´ï¼‰
        year_count = {}
        for paper in papers:
            if paper['published']:
                try:
                    year = paper['published'][:4]  # å–å¹´ä»½
                    year_count[year] = year_count.get(year, 0) + 1
                except:
                    pass

        return {
            "total_papers": len(papers),
            "categories": category_count,
            "years": year_count,
            "latest_fetch": papers[-1].get('created_at') if papers else None
        }

def fetch_and_save_papers(categories: List[str] = ["cs.CV", "cs.AI", "cs.LG"],
                         max_results: int = 20,
                         csv_filename: str = "arxiv_papers.csv"):
    """
    è·å–å¹¶ä¿å­˜ arXiv è®ºæ–‡åˆ° CSV çš„ä¾¿æ·å‡½æ•°

    Args:
        categories: è®ºæ–‡ç±»åˆ«åˆ—è¡¨
        max_results: æœ€å¤§è·å–æ•°é‡
        csv_filename: CSV æ–‡ä»¶å
    """
    print("ğŸš€ arXiv è®ºæ–‡è‡ªåŠ¨è·å–å’Œå­˜å‚¨å·¥å…·")
    print("=" * 50)

    # åˆ›å»ºè·å–å™¨
    fetcher = ArxivFetcher(delay_seconds=2)

    # è·å–æœ€æ–°è®ºæ–‡
    print(f"ğŸ“¡ æ­£åœ¨è·å– arXiv è®ºæ–‡...")
    papers = fetcher.fetch_latest_papers(
        categories=categories,
        max_results=max_results
    )

    if not papers:
        print("âŒ æœªè·å–åˆ°è®ºæ–‡æ•°æ®")
        return

    # ä¿å­˜åˆ° CSV
    csv_saver = ArxivToCSV(csv_filename)
    csv_saver.save_to_csv(papers, append_mode=True)

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = csv_saver.get_statistics()
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»è®ºæ–‡æ•°: {stats['total_papers']}")
    print(f"   ä¸»è¦ç±»åˆ«: {dict(list(stats.get('categories', {}).items())[:5])}")

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    fetch_and_save_papers(
        categories=["cs.CV", "cs.AI", "cs.LG"],
        max_results=10,
        csv_filename="arxiv_papers.csv"
    )

    # æœç´¢ç¤ºä¾‹
    print("\nğŸ” æœç´¢ç¤ºä¾‹:")
    csv_saver = ArxivToCSV("arxiv_papers.csv")
    search_results = csv_saver.search_in_csv("transformer")

    if search_results:
        for i, paper in enumerate(search_results[:3], 1):
            print(f"{i}. {paper['title'][:80]}...")
            print(f"   ä½œè€…: {paper['authors'][0]} ç­‰" if paper['authors'] else "   ä½œè€…: æœªçŸ¥")
            print(f"   ç±»åˆ«: {', '.join(paper['categories'][:3])}")
            print()