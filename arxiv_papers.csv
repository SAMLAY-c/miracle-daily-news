arxiv_id,title,authors,summary,published_date,updated_date,categories,pdf_url,source_url,created_at
2512.02020,EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI,Jianlei Chang; Ruofeng Mei; Wei Ke; Xiangyu Xu,"Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.",2025-12-01T18:59:59Z,2025-12-01T18:59:59Z,cs.RO; cs.AI; cs.CV; cs.LG,https://arxiv.org/pdf/2512.02020v1,http://arxiv.org/abs/2512.02020v1,2025-12-02 23:35:18
2512.02019,A Diffusion Model Framework for Maximum Entropy Reinforcement Learning,Sebastian Sanokowski; Kaustubh Patil; Alois Knoll,"Diffusion models have achieved remarkable success in data-driven learning and in sampling from complex, unnormalized target distributions. Building on this progress, we reinterpret Maximum Entropy Reinforcement Learning (MaxEntRL) as a diffusion model-based sampling problem. We tackle this problem by minimizing the reverse Kullback-Leibler (KL) divergence between the diffusion policy and the optimal policy distribution using a tractable upper bound. By applying the policy gradient theorem to this objective, we derive a modified surrogate objective for MaxEntRL that incorporates diffusion dynamics in a principled way. This leads to simple diffusion-based variants of Soft Actor-Critic (SAC), Proximal Policy Optimization (PPO) and Wasserstein Policy Optimization (WPO), termed DiffSAC, DiffPPO and DiffWPO. All of these methods require only minor implementation changes to their base algorithm. We find that on standard continuous control benchmarks, DiffSAC, DiffPPO and DiffWPO achieve better returns and higher sample efficiency than SAC and PPO.",2025-12-01T18:59:58Z,2025-12-01T18:59:58Z,cs.LG; cs.AI; stat.ML,https://arxiv.org/pdf/2512.02019v1,http://arxiv.org/abs/2512.02019v1,2025-12-02 23:35:18
2512.02018,Data-Centric Visual Development for Self-Driving Labs,Anbang Liu; Guanzhong Hu; Jiayi Wang; Ping Guo; Han Liu,"Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.",2025-12-01T18:59:57Z,2025-12-01T18:59:57Z,cs.CV; cs.RO,https://arxiv.org/pdf/2512.02018v1,http://arxiv.org/abs/2512.02018v1,2025-12-02 23:35:18
2512.02017,Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion,Shaowei Liu; David Yifan Yao; Saurabh Gupta; Shenlong Wang,"Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",2025-12-01T18:59:57Z,2025-12-01T18:59:57Z,cs.CV; cs.AI; cs.LG; cs.RO,https://arxiv.org/pdf/2512.02017v1,http://arxiv.org/abs/2512.02017v1,2025-12-02 23:35:18
2512.02016,Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now,Varun Varma Thozhiyoor; Shivam Tripathi; Venkatesh Babu Radhakrishnan; Anand Bhattad,"Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio $t_1^2/t_2^2 = h_1/h_2$, a relationship independent of $g$, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises $g_{\mathrm{eff}}$ from $1.81\,\mathrm{m/s^2}$ to $6.43\,\mathrm{m/s^2}$ (reaching $65\%$ of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.",2025-12-01T18:59:56Z,2025-12-01T18:59:56Z,cs.CV,https://arxiv.org/pdf/2512.02016v1,http://arxiv.org/abs/2512.02016v1,2025-12-02 23:35:18
2512.02015,Generative Video Motion Editing with 3D Point Tracks,Yao-Chih Lee; Zhoutong Zhang; Jiahui Huang; Jui-Hsien Wang; Joon-Young Lee; Jia-Bin Huang; Eli Shechtman; Zhengqi Li,"Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.",2025-12-01T18:59:55Z,2025-12-01T18:59:55Z,cs.CV,https://arxiv.org/pdf/2512.02015v1,http://arxiv.org/abs/2512.02015v1,2025-12-02 23:35:18
2512.02014,TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models,Zhiheng Liu; Weiming Ren; Haozhe Liu; Zijian Zhou; Shoufa Chen; Haonan Qiu; Xiaoke Huang; Zhaochong An; Fanny Yang; Aditya Patel; Viktar Atliha; Tony Ng; Xiao Han; Chuyan Zhu; Chenyang Zhang; Ding Liu; Juan-Manuel Perez-Rua; Sen He; JÃ¼rgen Schmidhuber; Wenhu Chen; Ping Luo; Wei Liu; Tao Xiang; Jonas Schult; Yuren Cong,"Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.",2025-12-01T18:59:51Z,2025-12-01T18:59:51Z,cs.CV,https://arxiv.org/pdf/2512.02014v1,http://arxiv.org/abs/2512.02014v1,2025-12-02 23:35:18
2512.02012,Improved Mean Flows: On the Challenges of Fastforward Generative Models,Zhengyang Geng; Yiyang Lu; Zongze Wu; Eli Shechtman; J. Zico Kolter; Kaiming He,"MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\textbf{improved MeanFlow}$ ($\textbf{iMF}$) method, trained entirely from scratch, achieves $\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.",2025-12-01T18:59:49Z,2025-12-01T18:59:49Z,cs.CV; cs.LG,https://arxiv.org/pdf/2512.02012v1,http://arxiv.org/abs/2512.02012v1,2025-12-02 23:35:18
2512.02010,Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling,Jack Cook; Junxian Guo; Guangxuan Xiao; Yujun Lin; Song Han,"As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.",2025-12-01T18:59:45Z,2025-12-01T18:59:45Z,cs.CL; cs.LG,https://arxiv.org/pdf/2512.02010v1,http://arxiv.org/abs/2512.02010v1,2025-12-02 23:35:18
2512.02009,AirSim360: A Panoramic Simulation Platform within Drone View,Xian Ge; Yuling Pan; Yuhang Zhang; Xiang Li; Weijun Zhang; Dizhe Zhang; Zhaoliang Wan; Xin Lin; Xiangkai Zhang; Juntao Liang; Jason Li; Wenjie Jiang; Bo Du; Ming-Hsuan Yang; Lu Qi,"The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.",2025-12-01T18:59:30Z,2025-12-01T18:59:30Z,cs.CV,https://arxiv.org/pdf/2512.02009v1,http://arxiv.org/abs/2512.02009v1,2025-12-02 23:35:18
