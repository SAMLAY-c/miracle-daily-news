import requests
import xml.etree.ElementTree as ET
import time
import os
import re
from typing import List, Dict, Optional, Set

class ArxivFetcher:
    """arXiv API æŸ¥è¯¢æ¨¡å—ï¼Œæ”¯æŒè®ºæ–‡èŽ·å–å’ŒåŽ»é‡"""

    def __init__(self, delay_seconds: int = 1):
        """
        åˆå§‹åŒ– arXiv èŽ·å–å™¨

        Args:
            delay_seconds: è¯·æ±‚é—´éš”æ—¶é—´ï¼Œéµå®ˆ arXiv 1ç§’1æ¬¡çš„é™åˆ¶
        """
        self.base_url = "http://export.arxiv.org/api/query"
        self.delay_seconds = delay_seconds
        self.namespace = {'atom': 'http://www.w3.org/2005/Atom'}
        self.processed_file = 'processed_arxiv_ids.txt'

    def _make_request(self, params: Dict) -> str:
        """å‘èµ· API è¯·æ±‚ï¼ŒåŒ…å«é€ŸçŽ‡é™åˆ¶"""
        time.sleep(self.delay_seconds)  # éµå®ˆé€ŸçŽ‡é™åˆ¶

        try:
            response = requests.get(self.base_url, params=params, timeout=30)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            raise Exception(f"API è¯·æ±‚å¤±è´¥: {e}")

    def _parse_arxiv_id(self, arxiv_url: str) -> str:
        """ä»Ž arXiv URL æå–è®ºæ–‡ ID"""
        # åŒ¹é…æ ¼å¼: http://arxiv.org/abs/2301.xxxxx æˆ– https://arxiv.org/abs/2301.xxxxx
        match = re.search(r'arxiv\.org/abs/(\d+\.\d+)', arxiv_url)
        return match.group(1) if match else arxiv_url

    def _load_processed_ids(self) -> Set[str]:
        """åŠ è½½å·²å¤„ç†çš„è®ºæ–‡ ID"""
        if not os.path.exists(self.processed_file):
            return set()

        try:
            with open(self.processed_file, 'r', encoding='utf-8') as f:
                return set(line.strip() for line in f if line.strip())
        except Exception as e:
            print(f"âš ï¸ è¯»å–å·²å¤„ç† ID æ–‡ä»¶å¤±è´¥: {e}")
            return set()

    def _save_processed_ids(self, processed_ids: Set[str]) -> None:
        """ä¿å­˜å·²å¤„ç†çš„è®ºæ–‡ ID"""
        try:
            with open(self.processed_file, 'w', encoding='utf-8') as f:
                for paper_id in sorted(processed_ids):
                    f.write(f"{paper_id}\n")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å·²å¤„ç† ID æ–‡ä»¶å¤±è´¥: {e}")

    def fetch_latest_papers(self,
                          categories: List[str] = ["cs.CV", "cs.AI", "cs.LG"],
                          max_results: int = 20,
                          days_back: int = 1) -> List[Dict]:
        """
        èŽ·å–æœ€æ–°çš„ arXiv è®ºæ–‡ï¼ˆå¸¦åŽ»é‡åŠŸèƒ½ï¼‰

        Args:
            categories: è®ºæ–‡ç±»åˆ«åˆ—è¡¨ï¼Œå¦‚ ["cs.CV", "cs.AI"]
            max_results: æœ€å¤§èŽ·å–æ•°é‡
            days_back: æŸ¥è¯¢æœ€è¿‘å‡ å¤©çš„è®ºæ–‡

        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        print(f"ðŸ“¡ æ­£åœ¨èŽ·å– arXiv æœ€æ–°è®ºæ–‡...")
        print(f"   ç±»åˆ«: {', '.join(categories)}")
        print(f"   æ•°é‡: {max_results}")

        # æž„å»ºæŸ¥è¯¢è¯­å¥
        category_query = " OR ".join([f"cat:{cat}" for cat in categories])

        params = {
            'search_query': category_query,
            'start': 0,
            'max_results': max_results * 2,  # èŽ·å–æ›´å¤šä»¥ä¾¿è¿‡æ»¤
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }

        try:
            # åŠ è½½å·²å¤„ç†çš„è®ºæ–‡ ID
            processed_ids = self._load_processed_ids()
            print(f"ðŸ“– å·²æ‰¾åˆ° {len(processed_ids)} ä¸ªåŽ†å²è®ºæ–‡è®°å½•")

            # å‘èµ·è¯·æ±‚
            xml_data = self._make_request(params)
            root = ET.fromstring(xml_data)

            papers = []
            new_papers_count = 0
            skipped_count = 0

            for entry in root.findall('atom:entry', self.namespace):
                if len(papers) >= max_results:
                    break

                try:
                    # è§£æžè®ºæ–‡ä¿¡æ¯
                    paper_info = self._parse_paper_entry(entry)
                    paper_id = self._parse_arxiv_id(paper_info['id'])

                    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
                    if paper_id in processed_ids:
                        skipped_count += 1
                        continue

                    # æ·»åŠ è®ºæ–‡ ID åˆ°å·²å¤„ç†é›†åˆ
                    paper_info['arxiv_id'] = paper_id
                    papers.append(paper_info)
                    processed_ids.add(paper_id)
                    new_papers_count += 1

                except Exception as e:
                    print(f"âš ï¸ è§£æžè®ºæ–‡æ¡ç›®æ—¶å‡ºé”™: {e}")
                    continue

            # ä¿å­˜æ›´æ–°åŽçš„å·²å¤„ç† ID åˆ—è¡¨
            if new_papers_count > 0:
                self._save_processed_ids(processed_ids)
                print(f"ðŸ’¾ å·²ä¿å­˜ {new_papers_count} ä¸ªæ–°è®ºæ–‡ ID")

            print(f"âœ… æˆåŠŸèŽ·å– {len(papers)} ç¯‡æ–°è®ºæ–‡ï¼ˆè·³è¿‡ {skipped_count} ç¯‡é‡å¤ï¼‰")
            return papers

        except Exception as e:
            print(f"âŒ èŽ·å– arXiv è®ºæ–‡å¤±è´¥: {e}")
            return []

    def _parse_paper_entry(self, entry) -> Dict:
        """è§£æžå•ä¸ªè®ºæ–‡æ¡ç›®"""
        # åŸºæœ¬ä¿¡æ¯
        paper_id = entry.find('atom:id', self.namespace).text
        title = entry.find('atom:title', self.namespace).text.strip()
        summary = entry.find('atom:summary', self.namespace).text.strip()

        # ä½œè€…ä¿¡æ¯
        authors = []
        for author in entry.findall('atom:author', self.namespace):
            name = author.find('atom:name', self.namespace).text.strip()
            authors.append(name)

        # æ—¶é—´ä¿¡æ¯
        published = entry.find('atom:published', self.namespace).text
        updated = entry.find('atom:updated', self.namespace).text

        # é“¾æŽ¥ä¿¡æ¯
        pdf_url = None
        for link in entry.findall('atom:link', self.namespace):
            if link.get('title') == 'pdf':
                pdf_url = link.get('href')
                break

        # æå–ç±»åˆ«ä¿¡æ¯
        categories = []
        for category in entry.findall('atom:category', self.namespace):
            term = category.get('term')
            if term:
                categories.append(term)

        return {
            'id': paper_id,
            'title': title,
            'summary': summary,
            'authors': authors,
            'published': published,
            'updated': updated,
            'pdf_url': pdf_url,
            'categories': categories,
            'source': 'arxiv'
        }

    def search_papers(self,
                     query: str,
                     max_results: int = 10,
                     sort_by: str = "relevance") -> List[Dict]:
        """
        æœç´¢ç‰¹å®šä¸»é¢˜çš„è®ºæ–‡

        Args:
            query: æœç´¢å…³é”®è¯ï¼Œå¦‚ "transformer attention"
            max_results: æœ€å¤§è¿”å›žæ•°é‡
            sort_by: æŽ’åºæ–¹å¼: "relevance", "lastUpdatedDate", "submittedDate"

        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        print(f"ðŸ” æ­£åœ¨æœç´¢ arXiv è®ºæ–‡: {query}")

        params = {
            'search_query': f'all:"{query}"',
            'start': 0,
            'max_results': max_results,
            'sortBy': sort_by,
            'sortOrder': 'descending'
        }

        try:
            xml_data = self._make_request(params)
            root = ET.fromstring(xml_data)

            papers = []
            for entry in root.findall('atom:entry', self.namespace):
                if len(papers) >= max_results:
                    break

                try:
                    paper_info = self._parse_paper_entry(entry)
                    papers.append(paper_info)
                except Exception as e:
                    print(f"âš ï¸ è§£æžè®ºæ–‡æ¡ç›®æ—¶å‡ºé”™: {e}")
                    continue

            print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡")
            return papers

        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []

    def get_paper_by_id(self, paper_id: str) -> Optional[Dict]:
        """æ ¹æ®è®ºæ–‡ ID èŽ·å–è¯¦ç»†ä¿¡æ¯"""
        params = {
            'search_query': f'id:{paper_id}',
            'start': 0,
            'max_results': 1
        }

        try:
            xml_data = self._make_request(params)
            root = ET.fromstring(xml_data)

            entries = root.findall('atom:entry', self.namespace)
            if entries:
                paper_info = self._parse_paper_entry(entries[0])
                paper_info['arxiv_id'] = paper_id
                return paper_info

            return None

        except Exception as e:
            print(f"âŒ èŽ·å–è®ºæ–‡ {paper_id} å¤±è´¥: {e}")
            return None

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # åˆ›å»ºèŽ·å–å™¨
    fetcher = ArxivFetcher(delay_seconds=2)  # 2ç§’é—´éš”ï¼Œä¿å®ˆèµ·è§

    # èŽ·å–æœ€æ–°è®ºæ–‡
    latest_papers = fetcher.fetch_latest_papers(
        categories=["cs.CV", "cs.AI"],
        max_results=5
    )

    print(f"\nðŸ“š æœ€æ–°è®ºæ–‡:")
    for paper in latest_papers:
        print(f"   æ ‡é¢˜: {paper['title'][:80]}...")
        print(f"   ä½œè€…: {', '.join(paper['authors'][:3])}{'ç­‰' if len(paper['authors']) > 3 else ''}")
        print(f"   ç±»åˆ«: {', '.join(paper['categories'][:3])}")
        print(f"   é“¾æŽ¥: {paper['id']}")
        print()