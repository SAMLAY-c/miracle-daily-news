import requests
import json
import time
import os
from dotenv import load_dotenv
from arxiv_fetcher import ArxivFetcher

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ================= é…ç½®åŒºåŸŸ =================

# 1. SiliconFlow (DeepSeek) é…ç½®
SILICON_KEY = os.getenv("SILICON_KEY", 'sk-keakcptlwtptnosbliohqompvsgxdtwctolxqjiwxddahyqk')
MODEL_NAME = "deepseek-ai/DeepSeek-V3"

# 2. é£ä¹¦æœºå™¨äººé…ç½®
FEISHU_APP_ID = os.getenv("FEISHU_APP_ID", 'cli_a9a5b41b8abf1ced')
FEISHU_APP_SECRET = os.getenv("FEISHU_APP_SECRET", 'M8azGTlTa9Aqwv19fdUZwge714CqFWD1')

# 3. é£ä¹¦å¤šç»´è¡¨æ ¼é…ç½® (æ ¹æ®ä½ æä¾›çš„ Base é“¾æ¥å¡«å…¥)
FEISHU_BITABLE_APP_TOKEN = os.getenv("FEISHU_BITABLE_APP_TOKEN", 'ddCZbBA7baN2SjsUt5McCnrnnsc')
FEISHU_TABLE_ID = os.getenv("FEISHU_BITABLE_TABLE_ID", 'tblb9sbMaoghEbWW')

NEWS_LIMIT = int(os.getenv("NEWS_LIMIT", 5))
ARXIV_LIMIT = int(os.getenv("ARXIV_LIMIT", 3))
ARXIV_CATEGORIES = os.getenv("ARXIV_CATEGORIES", "cs.CV,cs.AI,cs.LG").split(",")

# ===========================================

# æç¤ºè¯æ¨¡æ¿
PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä½åƒé™†å¥‡åšå£«ä¸€æ ·æ•é”çš„é£é™©æŠ•èµ„äººã€‚è¯·é˜…è¯»ä»¥ä¸‹ç§‘æŠ€æ–°é—»æ ‡é¢˜ã€‚
è¯·è¿›è¡Œæ·±åº¦åˆ†æï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¿”å›ç»“æœï¼ˆä¸è¦è¿”å› Markdown ä»£ç å—ï¼‰ã€‚

éœ€è¦åˆ†æçš„ç»´åº¦ï¼ˆJSON Key å¿…é¡»ä¸¥æ ¼ä¸€è‡´ï¼‰ï¼š
1. "summary": ä¸€å¥è¯æ‘˜è¦ï¼ˆä¸­æ–‡ï¼Œ50å­—å†…ï¼‰ã€‚
2. "category": æ‰€å±é¢†åŸŸï¼Œå¿…é¡»ä»ä»¥ä¸‹é€‰é¡¹ä¸­ä¸¥æ ¼é€‰æ‹©ä¸€ä¸ªï¼š
   ["Generative AI", "SaaS", "ç¡¬ç§‘æŠ€", "å¼€å‘å·¥å…·", "Web3", "ç”Ÿç‰©ç§‘æŠ€", "å…¶ä»–"]
3. "reason": åº•å±‚é€»è¾‘ã€‚è¿™ä»¶äº‹ä¸ºä»€ä¹ˆå‘ç”Ÿï¼Ÿè§£å†³äº†ä»€ä¹ˆæœ¬è´¨é—®é¢˜ï¼Ÿï¼ˆä¸­æ–‡ï¼Œ50å­—å†…ï¼‰ã€‚
4. "impact": æ½œåœ¨å½±å“ã€‚å¯¹è¡Œä¸šæˆ–å¼€å‘è€…æ„å‘³ç€ä»€ä¹ˆï¼Ÿï¼ˆä¸­æ–‡ï¼Œ50å­—å†…ï¼‰ã€‚
5. "commercial_score": å•†ä¸šè½åœ°æ½œåŠ›è¯„åˆ†ï¼Œè¿”å›æ•´æ•° 1 åˆ° 5ï¼ˆ5ä¸ºæœ€é«˜ï¼‰ã€‚
6. "recommendation": æ¨èæŒ‡æ•°ï¼Œå¿…é¡»ä»ä»¥ä¸‹é€‰é¡¹ä¸­ä¸¥æ ¼é€‰æ‹©ä¸€ä¸ªï¼š
   ["ğŸ”¥ å¿…è¯»", "ğŸ‘€ å€¼å¾—å…³æ³¨", "â˜•ï¸ éšä¾¿çœ‹çœ‹"]

æ–°é—»æ ‡é¢˜ï¼š{title}
"""

# arXiv è®ºæ–‡ä¸“ç”¨æç¤ºè¯æ¨¡æ¿
ARXIV_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„AIç ”ç©¶å‘˜å’ŒæŠ€æœ¯æŠ•èµ„é¡¾é—®ã€‚è¯·åˆ†æä»¥ä¸‹ arXiv è®ºæ–‡ä¿¡æ¯ï¼Œè¯„ä¼°å…¶æŠ€æœ¯ä»·å€¼å’Œå•†ä¸šæ½œåŠ›ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¿”å›ç»“æœï¼ˆä¸è¦è¿”å› Markdown ä»£ç å—ï¼‰ã€‚

éœ€è¦åˆ†æçš„ç»´åº¦ï¼ˆJSON Key å¿…é¡»ä¸¥æ ¼ä¸€è‡´ï¼‰ï¼š
1. "summary": æŠ€æœ¯è´¡çŒ®æ‘˜è¦ï¼ˆä¸­æ–‡ï¼Œ50å­—å†…ï¼‰ã€‚
2. "category": æ‰€å±é¢†åŸŸï¼Œå¿…é¡»ä»ä»¥ä¸‹é€‰é¡¹ä¸­ä¸¥æ ¼é€‰æ‹©ä¸€ä¸ªï¼š
   ["Generative AI", "è®¡ç®—æœºè§†è§‰", "è‡ªç„¶è¯­è¨€å¤„ç†", "æœºå™¨å­¦ä¹ ", "å¼ºåŒ–å­¦ä¹ ", "å…¶ä»–"]
3. "innovation": æŠ€æœ¯åˆ›æ–°æ€§è¯„ä¼°ï¼Œç®€è¿°å…¶æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼ˆä¸­æ–‡ï¼Œ50å­—å†…ï¼‰ã€‚
4. "practicality": å®ç”¨æ€§è¯„ä¼°ï¼ŒæŠ€æœ¯è½åœ°çš„å¯è¡Œæ€§å’Œéš¾åº¦ï¼ˆä¸­æ–‡ï¼Œ50å­—å†…ï¼‰ã€‚
5. "commercial_score": å•†ä¸šæ½œåŠ›è¯„åˆ†ï¼Œè¿”å›æ•´æ•° 1 åˆ° 5ï¼ˆ5ä¸ºæœ€é«˜ï¼‰ã€‚
6. "recommendation": æ¨èæŒ‡æ•°ï¼Œå¿…é¡»ä»ä»¥ä¸‹é€‰é¡¹ä¸­ä¸¥æ ¼é€‰æ‹©ä¸€ä¸ªï¼š
   ["ğŸ”¥ é‡å¤§çªç ´", "ğŸ‘€ é‡è¦è¿›å±•", "â˜•ï¸ å­¦æœ¯ä»·å€¼"]

è®ºæ–‡æ ‡é¢˜ï¼š{title}
è®ºæ–‡æ‘˜è¦ï¼š{summary}
ä½œè€…ï¼š{authors}
ç±»åˆ«ï¼š{categories}
"""

def get_tenant_token():
    """è·å–é£ä¹¦æœºå™¨äººè®¿é—®å‡­è¯"""
    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
    payload = {"app_id": FEISHU_APP_ID, "app_secret": FEISHU_APP_SECRET}
    try:
        resp = requests.post(url, json=payload)
        if resp.status_code != 200:
            print(f"âŒ æœºå™¨äºº Token è·å–å¤±è´¥: {resp.text}")
            return None
        return resp.json().get("tenant_access_token")
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        return None

def get_hn_news(limit=NEWS_LIMIT):
    """æŠ“å– Hacker News çƒ­é—¨æ–°é—»ï¼ˆå¸¦å»é‡åŠŸèƒ½ï¼‰"""
    print(f"ğŸ“¡ æ­£åœ¨æŠ“å– Top {limit} æ¡æ–°é—»...")
    try:
        # è¯»å–å·²å¤„ç†è¿‡çš„æ–°é—»æ ‡é¢˜æ–‡ä»¶
        processed_titles = set()
        processed_file = 'processed_hacker_news_titles.txt'

        if os.path.exists(processed_file):
            print(f"ğŸ“– å·²æ‰¾åˆ°å†å²è®°å½•ï¼Œè¯»å– {processed_file}")
            with open(processed_file, 'r', encoding='utf-8') as f:
                processed_titles = set(line.strip() for line in f if line.strip())

        # è·å–æ›´å¤šæ–°é—»ä»¥ä¾¿è¿‡æ»¤
        top_ids = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json").json()[:limit * 2]  # è·å–æ›´å¤šä»¥è¿‡æ»¤
        stories = []
        skipped_count = 0
        processed_count = 0

        for tid in top_ids:
            if len(stories) >= limit:  # å·²è¾¾åˆ°ç›®æ ‡æ•°é‡
                break

            try:
                item = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{tid}.json").json()
                if item and item.get('url') and item.get('title'):
                    title = item.get('title', '').strip()
                    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
                    if title not in processed_titles:
                        stories.append(item)
                        # å®æ—¶æ·»åŠ åˆ°å·²å¤„ç†åˆ—è¡¨
                        processed_titles.add(title)
                    else:
                        skipped_count += 1
                        processed_count += 1
            except Exception as e:
                print(f"âš ï¸ è·å–æ–°é—» {tid} æ—¶å‡ºé”™: {e}")
                continue

        # ä¿å­˜æ–°å¤„ç†çš„æ ‡é¢˜åˆ°æ–‡ä»¶
        if stories:
            new_titles = [item.get('title', '').strip() for item in stories]
            print(f"ğŸ’¾ ä¿å­˜ {len(new_titles)} ä¸ªæ–°æ ‡é¢˜åˆ°å†å²è®°å½•")
            with open(processed_file, 'w', encoding='utf-8') as f:
                for title in sorted(processed_titles.union(new_titles)):
                    f.write(f"{title}\n")

        print(f"âœ… æˆåŠŸè·å– {len(stories)} æ¡æ–°æ•°æ®ï¼ˆè·³è¿‡ {processed_count} æ¡é‡å¤ï¼‰")

        return stories[:limit]  # ç¡®ä¿è¿”å›æ­£ç¡®çš„æ•°é‡
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        return []

def get_arxiv_papers(limit=ARXIV_LIMIT, categories=ARXIV_CATEGORIES):
    """è·å–æœ€æ–° arXiv è®ºæ–‡ï¼ˆå¸¦å»é‡åŠŸèƒ½ï¼‰"""
    print(f"ğŸ“š æ­£åœ¨è·å– arXiv æœ€æ–°è®ºæ–‡...")
    try:
        fetcher = ArxivFetcher(delay_seconds=2)  # 2ç§’é—´éš”ï¼Œéµå®ˆé€Ÿç‡é™åˆ¶
        papers = fetcher.fetch_latest_papers(
            categories=categories,
            max_results=limit
        )

        # ä¸ºæ¯ä¸ªè®ºæ–‡æ·»åŠ æ¥æºæ ‡è¯†
        for paper in papers:
            paper['source'] = 'arxiv'

        return papers

    except Exception as e:
        print(f"âŒ è·å– arXiv è®ºæ–‡å¤±è´¥: {e}")
        return []

def analyze_and_write(news_items, token):
    """AI åˆ†æå¹¶å†™å…¥é£ä¹¦"""
    # é£ä¹¦å†™å…¥æ¥å£
    feishu_url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{FEISHU_BITABLE_APP_TOKEN}/tables/{FEISHU_TABLE_ID}/records/batch_create"
    feishu_headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}

    # DeepSeek æ¥å£
    ai_url = "https://api.siliconflow.cn/v1/chat/completions"
    ai_headers = {"Authorization": f"Bearer {SILICON_KEY}", "Content-Type": "application/json"}

    success_count = 0
    failed_count = 0
    records = []

    for item in news_items:
        title = item.get('title', 'æ— æ ‡é¢˜')
        source = item.get('source', 'unknown')
        print(f"\nğŸ§  æ­£åœ¨åˆ†æ: {title[:40]}...")

        try:
            # é€‰æ‹©æ­£ç¡®çš„æç¤ºè¯æ¨¡æ¿
            if source == 'arxiv':
                # arXiv è®ºæ–‡ä¸“ç”¨æ¨¡æ¿
                prompt = ARXIV_PROMPT_TEMPLATE.format(
                    title=title,
                    summary=item.get('summary', '')[:200],
                    authors=', '.join(item.get('authors', [])[:3]),
                    categories=', '.join(item.get('categories', [])[:3])
                )
            else:
                # Hacker News é€šç”¨æ¨¡æ¿
                prompt = PROMPT_TEMPLATE.format(title=title)

            # è°ƒç”¨ AI
            payload = {
                "model": MODEL_NAME,
                "messages": [{"role": "user", "content": prompt}],
                "response_format": {"type": "json_object"},
                "temperature": 0.3
            }

            ai_resp = requests.post(ai_url, headers=ai_headers, json=payload)
            ai_data = ai_resp.json()

            # è§£æ AI è¿”å›çš„å†…å®¹
            content_str = ai_data['choices'][0]['message']['content']
            analysis = json.loads(content_str)

            # æ„é€ é£ä¹¦æ•°æ® Payload (ä½¿ç”¨æ­£ç¡®çš„å­—æ®µID)
            current_time_ms = int(time.time() * 1000)

            fields = {
                "fldRJ6ZXT2": current_time_ms,  # æ”¶è—æ—¥æœŸ
                "fldQySf922": title,  # æ–°é—»æ ‡é¢˜/è®ºæ–‡æ ‡é¢˜
                "fldhcSKytX": int(time.time() * 1000),  # å‘å¸ƒæ—¥æœŸ
                "fld0fcfgz0": item.get('id') or item.get('url'),  # åŸæ–‡é“¾æ¥
                "fld7j1isdW": item.get('score', 0) if source != 'arxiv' else 0,  # HNçƒ­åº¦ (arXivè®¾ä¸º0)
                "fldkkjQi8y": analysis.get('category', 'å…¶ä»–'),  # æ‰€å±é¢†åŸŸ
                "fldom51JuS": analysis.get('summary', 'åˆ†æå¤±è´¥'),  # ä¸€å¥è¯æ‘˜è¦
                "fld0RXbCrS": analysis.get('reason', analysis.get('innovation', '')),  # åº•å±‚é€»è¾‘/æŠ€æœ¯åˆ›æ–°æ€§
                "fld0vyHCr2": analysis.get('impact', analysis.get('practicality', '')),  # æ½œåœ¨å½±å“/å®ç”¨æ€§è¯„ä¼°
                "fldwYrkaCR": analysis.get('recommendation', 'â˜•ï¸ éšä¾¿çœ‹çœ‹'),  # AIæ¨è
                "fldhwToUil": analysis.get('commercial_score', 3)  # å•†ä¸šæ½œåŠ›
            }

            records.append({"fields": fields})

            print(f"   âœ… [åˆ†æå®Œæˆ] å•†ä¸šæ½œåŠ›: {analysis.get('commercial_score')}æ˜Ÿ | {analysis.get('recommendation')}")
            success_count += 1

        except Exception as e:
            print(f"   âŒ å¤„ç†å‡ºé”™: {e}")
            failed_count += 1

        # é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(1)

    # æ‰¹é‡å†™å…¥é£ä¹¦
    if records:
        try:
            write_payload = {"records": records}
            write_resp = requests.post(feishu_url, headers=feishu_headers, json=write_payload)
            write_res = write_resp.json()

            if write_res.get('code') == 0:
                print(f"\nğŸ’¾ [æ‰¹é‡å†™å…¥æˆåŠŸ] æˆåŠŸå†™å…¥ {len(records)} æ¡è®°å½•")
            else:
                print(f"\nâŒ [æ‰¹é‡å†™å…¥å¤±è´¥] {write_res.get('msg')}")
                if 'data' in write_res and write_res['data']:
                    print("é”™è¯¯è¯¦æƒ…:", write_res['data'])
        except Exception as e:
            print(f"\nâŒ æ‰¹é‡å†™å…¥å‡ºé”™: {e}")

    print(f"\nğŸ‰ ä»»åŠ¡ç»“æŸï¼")
    print(f"   âœ… æˆåŠŸåˆ†æ: {success_count} æ¡æ–°é—»")
    print(f"   âŒ å¤±è´¥: {failed_count} æ¡æ–°é—»")
    print(f"   ğŸ“Š å¤„ç†æ•ˆç‡: {success_count}/{len(news_items)}")

def show_processed_history():
    """æ˜¾ç¤ºå·²å¤„ç†çš„å†å²è®°å½•"""
    processed_file = 'processed_hacker_news_titles.txt'
    if os.path.exists(processed_file):
        print(f"ğŸ“– {processed_file} ä¸­çš„å·²å¤„ç†æ ‡é¢˜:")
        print("=" * 50)
        with open(processed_file, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f, 1):
                if line.strip():
                    print(f"{i:3d}. {line.strip()}")
        print("=" * 50)
    else:
        print(f"ğŸ“– {processed_file} ä¸å­˜åœ¨ï¼Œè¿™æ˜¯é¦–æ¬¡è¿è¡Œ")

if __name__ == "__main__":
    print("ğŸš€ Hacker News + arXiv è®ºæ–‡é£ä¹¦è‡ªåŠ¨åŒ–æƒ…æŠ¥ç«™ (å»é‡ç‰ˆ)")
    print("=" * 60)

    # æ˜¾ç¤ºå†å²è®°å½•
    show_processed_history()
    print()

    # 1. è·å– Token
    t_token = get_tenant_token()

    if t_token:
        # 2. è·å– Hacker News æ–°é—»ï¼ˆå¸¦å»é‡ï¼‰
        print("ğŸ“° è·å– Hacker News...")
        hn_list = get_hn_news(limit=NEWS_LIMIT)

        # 3. è·å– arXiv è®ºæ–‡ï¼ˆå¸¦å»é‡ï¼‰
        print("ğŸ“š è·å– arXiv è®ºæ–‡...")
        arxiv_list = get_arxiv_papers(limit=ARXIV_LIMIT, categories=ARXIV_CATEGORIES)

        # åˆå¹¶æ•°æ®
        all_items = []
        if hn_list:
            for item in hn_list:
                item['source'] = 'hacker_news'
                all_items.append(item)

        if arxiv_list:
            all_items.extend(arxiv_list)

        if all_items:
            print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
            print(f"   Hacker News: {len(hn_list)} æ¡")
            print(f"   arXiv è®ºæ–‡: {len(arxiv_list)} æ¡")
            print(f"   æ€»è®¡: {len(all_items)} æ¡")
            print()

            # 4. åˆ†æ + å†™å…¥
            analyze_and_write(all_items, t_token)
        else:
            print("âš ï¸ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
    else:
        print("âš ï¸ æ— æ³•è¿æ¥é£ä¹¦ï¼Œè¯·æ£€æŸ¥ App ID å’Œ Secret")