#!/usr/bin/env python3
"""
Arxiv æŠ“å–è„šæœ¬æµ‹è¯•ç‰ˆæœ¬
ç”¨äºéªŒè¯è¿æ¥å’ŒåŠŸèƒ½
"""

import requests
import xml.etree.ElementTree as ET
import json
import time

# é…ç½®
API_KEY = "t-g104c303A6373MHT63OJMF6KSKG4SWVPZU4D47NU"
APP_TOKEN = "DdCZbBA7baN2SjsUt5McCnrnnsc"
TABLE_ID = "tblb9sbMaoghEbWW"

def test_arxiv_connection():
    """æµ‹è¯•Arxiv APIè¿æ¥"""
    print("ğŸ”— æµ‹è¯•Arxiv APIè¿æ¥...")

    url = "http://export.arxiv.org/api/query?search_query=cat:cs.CV&start=0&max_results=5&sortBy=submittedDate&sortOrder=descending"

    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        print("âœ… Arxiv APIè¿æ¥æˆåŠŸ")
        return response.text
    except Exception as e:
        print(f"âŒ Arxiv APIè¿æ¥å¤±è´¥: {e}")
        return None

def parse_simple_xml(xml_data):
    """ç®€åŒ–ç‰ˆXMLè§£æ"""
    try:
        root = ET.fromstring(xml_data)
        ns = {'atom': 'http://www.w3.org/2005/Atom', 'arxiv': 'http://arxiv.org/schemas/atom'}

        papers = []
        for entry in root.findall('atom:entry', ns):
            # åªè·å–åŸºæœ¬å­—æ®µ
            title = entry.find('atom:title', ns).text.strip().replace('\n', ' ')
            arxiv_id = entry.find('atom:id', ns).text.split('/')[-1]
            summary = entry.find('atom:summary', ns).text.strip()[:200] + "..."  # æˆªæ–­æ‘˜è¦

            # ä½œè€…ï¼ˆæœ€å¤š3ä¸ªï¼‰
            authors = []
            for author in entry.findall('atom:author', ns):
                authors.append(author.find('atom:name', ns).text)
                if len(authors) >= 3:
                    break

            # æ—¥æœŸï¼ˆç®€åŒ–å¤„ç†ï¼‰
            published = entry.find('atom:published', ns).text[:10]
            published_timestamp = int(time.time()) * 1000  # ç®€åŒ–ä¸ºå½“å‰æ—¶é—´

            paper = {
                'title': title,
                'arxiv_id': arxiv_id,
                'summary': summary,
                'authors': ", ".join(authors),
                'published': published_timestamp,
                'research_field': 'CV (è®¡ç®—æœºè§†è§‰)',  # å›ºå®šä¸ºCVè¿›è¡Œæµ‹è¯•
                'paper_url': f"https://arxiv.org/abs/{arxiv_id}"
            }
            papers.append(paper)

        print(f"âœ… æˆåŠŸè§£æ {len(papers)} ç¯‡è®ºæ–‡")
        return papers

    except Exception as e:
        print(f"âŒ XMLè§£æå¤±è´¥: {e}")
        return []

def create_feishu_record(paper):
    """åˆ›å»ºå•æ¡é£ä¹¦è®°å½•"""
    return {
        "fields": {
            "è®ºæ–‡æ ‡é¢˜": paper['title'],
            "æ‘˜è¦": paper['summary'],
            "ä½œè€…": paper['authors'],
            "Arxiv ID": paper['arxiv_id'],
            "å‘å¸ƒæ—¶é—´": paper['published'],
            "æ›´æ–°æ—¶é—´": paper['published'],  # ä½¿ç”¨å‘å¸ƒæ—¶é—´ä½œä¸ºæ›´æ–°æ—¶é—´
            "ç ”ç©¶é¢†åŸŸ": paper['research_field'],
            "å­¦ä¹ çŠ¶æ€": "å¾…è¯»",
            "åŸæ–‡é“¾æ¥": {
                "text": "Arxiv Link",
                "link": paper['paper_url']
            },
            "DOI": "",
            "æœŸåˆŠå¼•ç”¨": "",
            "å­¦ä¹ ç¬”è®°": ""
        }
    }

def test_feishu_write():
    """æµ‹è¯•å†™å…¥é£ä¹¦è¡¨æ ¼"""
    print("ğŸ“ æµ‹è¯•å†™å…¥é£ä¹¦è¡¨æ ¼...")

    # 1. æµ‹è¯•Arxivè¿æ¥
    xml_data = test_arxiv_connection()
    if not xml_data:
        return False

    # 2. è§£ææ•°æ®
    papers = parse_simple_xml(xml_data)
    if not papers:
        return False

    # 3. åˆ›å»ºé£ä¹¦è®°å½•ï¼ˆåªæµ‹è¯•å‰2ç¯‡ï¼‰
    records = []
    for paper in papers[:2]:  # åªæµ‹è¯•å‰2ç¯‡
        record = create_feishu_record(paper)
        records.append(record)

    # 4. å†™å…¥é£ä¹¦
    url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{APP_TOKEN}/tables/{TABLE_ID}/records/batch_create"
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json; charset=utf-8"
    }

    payload = {"records": records}

    try:
        print(f"ğŸš€ æ­£åœ¨å†™å…¥ {len(records)} æ¡æµ‹è¯•è®°å½•...")
        response = requests.post(url, headers=headers, data=json.dumps(payload, ensure_ascii=False).encode('utf-8'))

        print(f"ğŸ“Š HTTPçŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ“„ å“åº”å†…å®¹: {response.text}")

        if response.status_code == 200:
            result = response.json()
            if result.get('code') == 0:
                records_created = len(result.get('data', {}).get('records', []))
                print(f"âœ… æˆåŠŸå†™å…¥ {records_created} æ¡è®°å½•")
                return True
            else:
                print(f"âŒ APIè¿”å›é”™è¯¯: {result}")
                return False
        else:
            print(f"âŒ HTTPè¯·æ±‚å¤±è´¥: {response.status_code}")
            return False

    except Exception as e:
        print(f"âŒ å†™å…¥é£ä¹¦å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å¼€å§‹Arxiv+é£ä¹¦é›†æˆæµ‹è¯•")
    print("=" * 50)

    success = test_feishu_write()

    print("=" * 50)
    if success:
        print("ğŸ‰ æµ‹è¯•æˆåŠŸï¼ç³»ç»Ÿå·¥ä½œæ­£å¸¸")
        print("ğŸ’¡ ç°åœ¨å¯ä»¥è¿è¡Œå®Œæ•´çš„ arxiv_feishu_fetcher.py è„šæœ¬")
    else:
        print("âš ï¸ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®")

if __name__ == "__main__":
    main()