#!/usr/bin/env python3
"""
arXiv è®ºæ–‡å®šæ—¶è·å–è„šæœ¬
å¯ä»¥å®šæœŸè¿è¡Œï¼Œè‡ªåŠ¨è·å–æœ€æ–°è®ºæ–‡å¹¶ä¿å­˜åˆ° CSV
"""

import os
import sys
import argparse
from datetime import datetime
from arxiv_to_csv import ArxivToCSV, fetch_and_save_papers

def print_banner():
    """æ‰“å°ç¨‹åºæ ‡é¢˜"""
    print("ğŸš€ arXiv è®ºæ–‡å®šæ—¶è·å–å™¨")
    print("=" * 50)
    print(f"â° è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

def fetch_by_category(categories: list, max_papers: int, csv_file: str):
    """æŒ‰ç±»åˆ«è·å–è®ºæ–‡"""
    print(f"ğŸ“š è·å–ç±»åˆ«: {', '.join(categories)}")
    print(f"ğŸ“Š æœ€å¤§æ•°é‡: {max_papers}")

    fetch_and_save_papers(
        categories=categories,
        max_results=max_papers,
        csv_filename=csv_file
    )

def search_and_save(keyword: str, max_papers: int, csv_file: str):
    """æœç´¢ç‰¹å®šå…³é”®è¯çš„è®ºæ–‡"""
    from arxiv_fetcher import ArxivFetcher

    print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
    print(f"ğŸ“Š æœ€å¤§æ•°é‡: {max_papers}")

    fetcher = ArxivFetcher(delay_seconds=2)
    papers = fetcher.search_papers(
        query=keyword,
        max_results=max_papers,
        sort_by="submittedDate"
    )

    if papers:
        # ä¸ºæœç´¢çš„è®ºæ–‡æ·»åŠ  arxiv_id
        for paper in papers:
            if 'arxiv_id' not in paper and 'id' in paper:
                # ä» URL ä¸­æå– arxiv_id
                import re
                match = re.search(r'arxiv\.org/abs/(\d+\.\d+)', paper['id'])
                if match:
                    paper['arxiv_id'] = match.group(1)

        csv_saver = ArxivToCSV(csv_file)
        csv_saver.save_to_csv(papers, append_mode=True)
    else:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")

def show_statistics(csv_file: str):
    """æ˜¾ç¤º CSV æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
    csv_saver = ArxivToCSV(csv_file)
    stats = csv_saver.get_statistics()

    print(f"ğŸ“Š {csv_file} ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»è®ºæ–‡æ•°: {stats['total_papers']}")

    if stats.get('categories'):
        print("   ç±»åˆ«åˆ†å¸ƒ:")
        for category, count in sorted(stats['categories'].items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"     {category}: {count}")

    if stats.get('years'):
        print("   å¹´ä»½åˆ†å¸ƒ:")
        for year, count in sorted(stats['years'].items()):
            print(f"     {year}: {count}")

    if stats.get('latest_fetch'):
        print(f"   æœ€åè·å–: {stats['latest_fetch']}")

def search_in_csv(keyword: str, csv_file: str):
    """åœ¨ CSV ä¸­æœç´¢è®ºæ–‡"""
    csv_saver = ArxivToCSV(csv_file)
    results = csv_saver.search_in_csv(keyword)

    if results:
        print(f"ğŸ” æ‰¾åˆ° {len(results)} ç¯‡ç›¸å…³è®ºæ–‡:")
        print("-" * 80)
        for i, paper in enumerate(results[:10], 1):  # åªæ˜¾ç¤ºå‰10ç¯‡
            print(f"{i:2d}. {paper['title'][:70]}...")
            authors = paper['authors'][:2] if paper['authors'] else []
            author_str = ', '.join(authors) + (' ç­‰' if len(paper['authors']) > 2 else '')
            print(f"     ä½œè€…: {author_str}")
            print(f"     æ—¥æœŸ: {paper['published'][:10] if paper['published'] else 'æœªçŸ¥'}")
            print(f"     é“¾æ¥: {paper.get('source_url', 'æ— ')}")
            print()
    else:
        print(f"ğŸ” æœªæ‰¾åˆ°åŒ…å« '{keyword}' çš„è®ºæ–‡")

def export_to_markdown(csv_file: str, output_file: str = None):
    """å¯¼å‡º CSV æ•°æ®åˆ° Markdown æ–‡ä»¶"""
    if not output_file:
        output_file = csv_file.replace('.csv', '_export.md')

    csv_saver = ArxivToCSV(csv_file)
    papers = csv_saver.load_from_csv()

    if not papers:
        print("âŒ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
        return

    # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
    papers.sort(key=lambda x: x.get('published', ''), reverse=True)

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# arXiv è®ºæ–‡é›†\n\n")
        f.write(f"*å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
        f.write(f"*æ€»è®¡è®ºæ–‡æ•°: {len(papers)}*\n\n")

        # æŒ‰ç±»åˆ«åˆ†ç»„
        categories = {}
        for paper in papers:
            for category in paper['categories']:
                if category not in categories:
                    categories[category] = []
                categories[category].append(paper)

        for category, category_papers in sorted(categories.items()):
            f.write(f"## {category} ({len(category_papers)} ç¯‡)\n\n")

            for paper in category_papers[:5]:  # æ¯ä¸ªç±»åˆ«æœ€å¤šæ˜¾ç¤º5ç¯‡
                f.write(f"### {paper['title']}\n\n")
                f.write(f"**ä½œè€…:** {', '.join(paper['authors'][:3])}{' ç­‰' if len(paper['authors']) > 3 else ''}\n\n")
                f.write(f"**æ‘˜è¦:** {paper['summary'][:200]}{'...' if len(paper['summary']) > 200 else ''}\n\n")
                f.write(f"**é“¾æ¥:** [arXiv]({paper.get('source_url', 'æ— ')})")
                if paper.get('pdf_url'):
                    f.write(f" | [PDF]({paper['pdf_url']})")
                f.write(f"\n\n")
                f.write("---\n\n")

    print(f"âœ… æˆåŠŸå¯¼å‡ºåˆ° {output_file}")

def main():
    parser = argparse.ArgumentParser(description='arXiv è®ºæ–‡å®šæ—¶è·å–å’Œç®¡ç†å·¥å…·')
    parser.add_argument('--categories', '-c',
                       default='cs.CV,cs.AI,cs.LG',
                       help='è®ºæ–‡ç±»åˆ«ï¼Œç”¨é€—å·åˆ†éš” (é»˜è®¤: cs.CV,cs.AI,cs.LG)')
    parser.add_argument('--max-papers', '-m',
                       type=int, default=20,
                       help='æœ€å¤§è·å–è®ºæ–‡æ•° (é»˜è®¤: 20)')
    parser.add_argument('--csv-file', '-f',
                       default='arxiv_papers.csv',
                       help='CSV æ–‡ä»¶å (é»˜è®¤: arxiv_papers.csv)')
    parser.add_argument('--search', '-s',
                       help='æœç´¢å…³é”®è¯')
    parser.add_argument('--stats', action='store_true',
                       help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--search-csv',
                       help='åœ¨ CSV æ–‡ä»¶ä¸­æœç´¢')
    parser.add_argument('--export-markdown',
                       help='å¯¼å‡ºåˆ° Markdown æ–‡ä»¶')

    args = parser.parse_args()

    print_banner()

    try:
        if args.stats:
            show_statistics(args.csv_file)

        elif args.search_csv:
            search_in_csv(args.search_csv, args.csv_file)

        elif args.export_markdown:
            export_to_markdown(args.csv_file, args.export_markdown)

        elif args.search:
            search_and_save(args.search, args.max_papers, args.csv_file)

        else:
            # é»˜è®¤è¡Œä¸ºï¼šæŒ‰ç±»åˆ«è·å–è®ºæ–‡
            categories = [cat.strip() for cat in args.categories.split(',')]
            fetch_by_category(categories, args.max_papers, args.csv_file)

            # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
            print("\n" + "=" * 50)
            show_statistics(args.csv_file)

    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()